---
title: 'Uncloaking Hidden Repeating Fast Radio Bursts With Unsupervised Machine Learning'
author: 'Bo Han Chen et. al. (2021)'
format: 
    revealjs:
        footer: 'Murthadza bin Aznam'
        logo: '../_assets/radio-cosmology-lab-logo.png'
        chalkboard: true
        theme: simple
        slide-number: true
bibliography: references.bib
output-file: 'slides'
---

# Introduction

## Meaning
### 'Hidden Repeating Fast Radio Bursts'
- Fast Radio Bursts (FRB) are known to have two types: repeating and non-repeating
- A repeating FRB are observed to repeat
- However, a non-repeating FRB **could potentially be an actual repeater** but its repetition is not yet observed.

## Meaning
### 'Unsupervised Machine Learning'
- Unsupervised machine learning is a type of machine learning algorithm that does not rely on labels (`True`/`False` or 'repeating'/'non-repeating').

:::{.callout-important}
The machine learning algorithms presented in this paper only processes the FRB features' values (such as dispersion measure and redshift) and not the labels (repeating vs non-repeating).
:::

## Meaning
### 'Unsupervised Machine Learning'
#### Examples
- Dimension Reduction: 
    A task that aims to reduce high dimensions of data into lower dimensions for better understanding of their relationships.
- Clustering:
    A task that aims to categorizes data points into clusters based on their distance.

## Objective
- Use dimensional reduction algorithm and clustering on the reduced dimension <mark>to identify possibly misclassified non-repeating FRBs</mark>.
    These 'possibly misclassified non-repeating FRBs' are referred to as 'repeater candidates'.

<!-- > Our goal is to <mark>map several observational and model-dependent parameters of each FRB to a 2D embedding plane</mark> by training the UMAP algorithm on the features of the training samples in CHIME/FRB dataset **and finally identify possibly misclassified non-repeating FRBs which in fact have latent features of repeating FRBs**. We define these possibly misclassified non-repeating FRBs as FRB _repeater candidates_ in our paper. -->

# Method

## Data Composition
### Sample
- This paper uses the CHIME/FRB First Data Release^[@thechimefrbcollaborationFirstCHIMEFRB2021]
- There are 594 FRB sub-bursts:
    * 501 non-repeating + 93 repeating
    * at a frequency between 400 MHz and 800 MHz
    * observed from 25th Jul 2018 to 1st Jul 2019

## Data Composition {.smaller #data-features}
### Features^[Machine learning uses the term 'features' but the paper uses the word 'parameters' here.]
::::::{.columns}
:::::{.column width="60%"}
#### Observational Features
::::{.columns}
:::{.column width="50%"}
1. Boxcar Width
1. Width of Sub-Burst
1. Flux
1. Fluence
1. Scattering Time
:::
:::{.column width="50%"}
6. Spectral Index
1. Spectral Running
1. Highest Frequency
1. Lowest Frequency
1. Peak Frequency
:::
::::
:::::
:::::{.column width="40%"}
#### Model Dependent Features^[Calculated in @hashimotoEnergyFunctionsFast2022]
11. Redshift
1. Radio energy
1. Rest-frame intrinsic duration
:::::
::::::

## Procedure
```{mermaid}
flowchart LR
    A[Preprocessing] --> B[Reduce Dimension]
    B --> C[Cluster Data]
    C --> D[Postprocessing]

    style A stroke:#f66,stroke-width:2px
```
### 1. Preprocessing
- Select features and calculate if necessary.
    
    üëà Refer the [previous slide](#data-features)
- Split training (90%) and testing (10%) dataset.

    Only the repeaters are split
- Remove `null` values.

## Procedure
```{mermaid}
flowchart LR
    A[Preprocessing] --> B[Reduce Dimension]
    B --> C[Cluster Data]
    C --> D[Postprocessing]

    style B stroke:#f66,stroke-width:2px
```
### 2. Reduce Dimension
Map the selected features into a 2 dimension embedding space using the UMAP^['Uniform Manifold Approximation and Projection' described in @mcinnesUMAPUniformManifold2018 and implemented in [https://github.com/lmcinnes/umap](https://github.com/lmcinnes/umap)] algorithm.

## Procedure
```{mermaid}
flowchart LR
    A[Preprocessing] --> B[Reduce Dimension]
    B --> C[Cluster Data]
    C --> D[Postprocessing]

    style C stroke:#f66,stroke-width:2px
```
### 3. Cluster Data
Cluster the data points on the UMAP space using the HDBSCAN^['Hierarchical Density-Based Spatial Clustering of Applications with Noise' described in @campelloDensityBasedClusteringBased2013 and implemented in [https://github.com/scikit-learn-contrib/hdbscan](https://github.com/scikit-learn-contrib/hdbscan)] algorithm.

## Procedure
```{mermaid}
flowchart LR
    A[Preprocessing] --> B[Reduce Dimension]
    B --> C[Cluster Data]
    C --> D[Postprocessing]

    style D stroke:#f66,stroke-width:2px
```
### 4. Postprocessing
- Cluster identification
- Cluster implication
 
## Results {.smaller}
### Dimension Reduction (UMAP)
::::{.columns}
:::{.column width="45%"}
![](./figures/umap.png)
:::
:::{.column width="55%" .incremental}
#### Highlights
1. The distribution of repeating and non-repeating FRBs are similar (no sign of over-fitting).
1. Repeaters and non-repeaters are concentrated within their respective clusters.
1. There are non-repeaters mixed in repeaters--concentrated clusters.
:::
::::

## Results {.smaller}
### Clustering (HDBSCAN)
::::{.columns}
:::{.column width="45%"}
![](./figures/umap-hdbscan.png)
:::
:::{.column width="55%" .incremental}
#### Highlights
1. The HDBSCAN found 9 clusters of FRB samples. 
1. 3 of them are assigned as repeater clusters.
    A cluster is assigned as a repeater cluster if it is populated with >10% repeater FRB.
1. **Non-repeaters inside a repeater cluster are identified as repeater candidate**. A total of 118/474 (39.7%) are identified as a repeater candidates.
1. Total repeater percentages might be higher (41.9%) than previously thought (~5%).
:::
::::

## Implications {.smaller}
### Highest Frequency vs Peak Frequency
::::{.columns}
:::{.column width="45%"}
![](./figures/high-vs-peak.png)
:::
:::{.column width="55%" .incremental}
#### Highlights
1. Repeaters (‚ùå) are already aligned along the diagonal.
1. Repeater candidates (‚ô¶Ô∏è) also align along the diagonal.
1. Non-repeaters (üü¶) concentrate below the diagonal.
:::
::::

## Implications {.smaller}
### Highest/Peak Frequency vs Fluence
::::{.columns}
:::{.column width="45%"}
![](./figures/freq-ratio-vs-fluence.png)
:::
:::{.column width="55%" .incremental}
#### Highlights
1. The line $x = 1.4$ neatly separate repeater clusters and non-repeater clusters.
1. All repeaters (‚ùå) and repeater candidates (‚ô¶Ô∏è) are located at the ratio $x<1.4$.
1. Most non-repeaters (üü¶) are located at the ratio $x>=1.4$ while some of them bleed through $x<1.4$.
1. Some FRBs form a straight line at $\text{Fluence}=50\,\text{Jy}\cdot\text{ms}$ but the cause is unknown.
:::
::::

:::{.aside}
The ratio between the highest and peak frequencies is used because the redshift effect on these two measurements is cancelled out.
:::

## Implications {.smaller}
### Width vs Spectral Running
::::{.columns}
:::{.column width="45%"}
![](./figures/width-vs-spectral-running.png)
:::
:::{.column width="55%" .incremental}
#### Highlights
1. The distribution of spectral running appears to have two main peaks: an upper peak around 0 ~ -10 and a lower peak between 0 and -150. 
1. A simple cut at ‚Äò-25‚Äô, as shown by the dashed line, can divide them effectively into two major peaks.
1. Repeaters (‚ùå) and repeater candidates (‚ô¶Ô∏è) make up the lower distribution.
1. Non-repeaters (üü¶) make up the upper distribution.
:::
::::

## Conclusion
1. Unsupervised machine learning can tell the difference between repeaters and non-repeaters even though it has no knowledge of the labels.
    Therefore, **repeaters and non-repeaters may have an underlying difference; including a difference in origin**.
1. Some non-repeating FRBs have latent repeater features which is revealed using UMAP--HDBSCAN.
    Different plots have revealed that the **repeater candidates shows similar behaviour with known repeaters**.
 
## Reproducibility {.smaller}
1. The data is available in @hashimotoEnergyFunctionsFast2022 and the clustering result is appended as supplimentary material.
1. The procedure is detailed enough for me to be able to reproduce similar result except for HDBSCAN's `minimum cluster size` parameter which was unspecified.

![](https://thaza-kun.github.io/sarjana/notebooks/02-UMAP-HDBSCAN_files/figure-html/cell-13-output-2.png)
 

## References {.smaller}
:::{#refs}
:::

# Thank You

## Progress Report

:::{width="100%"}
```{python}
#| echo: false

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
import datetime as dt

sns.set_theme()

data = pd.read_csv('progress.csv')

data.start = pd.to_datetime(data.start)
data.end = pd.to_datetime(data.end)

# Add duration
data.loc[:, ['duration']] = (data.end - data.start).apply(lambda x: x.days + 1)

# Sort in ascending order
data = data.sort_values(by='start', ascending=True)

thesis_start = data.start.min()
thesis_end = data.end.max()
thesis_duration = (thesis_end-thesis_start).days + 1

# Add relative start
data.loc[:, ['rel_start']] = data.start.apply(lambda x: (x-thesis_start).days)

#Create custom x-ticks and x-tick labels
x_ticks=[i for i in range(thesis_start.day, thesis_duration+1)]
x_labels=[(thesis_start + dt.timedelta(days=i)) for i in x_ticks]

ticks = pd.DataFrame(data= {'ticks': x_ticks, 'labels': x_labels})
tick_interval = ticks[ticks.labels.apply(lambda x: x.is_month_start)].ticks
tick_labels = ticks[ticks.labels.apply(lambda x: x.is_month_start)].labels.apply(lambda x: x.strftime('%b %Y'))

now = dt.date.today().day + thesis_start.day

plt.figure(figsize=(15,5))
plt.title('Masters Study Progress')
plt.barh(y=data.task, left=data.rel_start, width=data.duration, color=data.color, label=data.phase)
plt.gca().invert_yaxis()
plt.axvline(x=now, color='orange')
plt.xticks(ticks=tick_interval, labels=tick_labels, rotation=90)
plt.grid(axis='y')

# Legends
legend_colors = {row.phase: row.color for row in data.itertuples()}
legend_elems = [Patch(facecolor=legend_colors[key], label=key) for key in legend_colors]
plt.legend(handles=legend_elems)

plt.show()
```
:::

## Publication Report {.smaller}
1. Is It Possible That Different Clusters Of Repeaters And Repeater Candidates Correspond To Different Mechanism Of Emission?
    - Status: Ideation
    - Methodology: M-W-W test can be used to test whether two samples might come from the same distribution (origin) as was done in [Cui, Xiang-Han et. al. (2021)](https://ui.adsabs.harvard.edu/abs/2021MNRAS.500.3275C/abstract)^['Fast radio bursts: do repeaters and non-repeaters originate in statistically similar ensembles?']
1. Is It Possible To Predict The Repeater Candidates' Feature Within Cluster Population?
    - Status: Ideation
    - Methodology: Using a prediction algorithm combined with a leave-out-out k